{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdcb28a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLEARML_WEB_HOST=https://app.clear.ml/\n",
      "env: CLEARML_API_HOST=https://api.clear.ml\n",
      "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
      "env: CLEARML_API_ACCESS_KEY=NLJ1C1S42IC6LW7YB4GQFOZXZJGJSC\n",
      "env: CLEARML_API_SECRET_KEY=qVctArALlJ5gxnUmmftsdLFO5mGdAhBtnw0IWcvF-5Hbhan-h4wEcHoTMjGCj19PKM4\n"
     ]
    }
   ],
   "source": [
    "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
    "%env CLEARML_API_HOST=https://api.clear.ml\n",
    "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
    "%env CLEARML_API_ACCESS_KEY=NLJ1C1S42IC6LW7YB4GQFOZXZJGJSC\n",
    "%env CLEARML_API_SECRET_KEY=qVctArALlJ5gxnUmmftsdLFO5mGdAhBtnw0IWcvF-5Hbhan-h4wEcHoTMjGCj19PKM4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03e30e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't get url information for git repo in C:\\Users\\maris\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\n",
      "Can't get branch information for git repo in C:\\Users\\maris\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=2b361086488e4955aa2ef6fc221b8945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't get commit information for git repo in C:\\Users\\maris\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\n",
      "Can't get diff information for git repo in C:\\Users\\maris\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML results page: https://app.clear.ml/projects/37329c57f7424fb081d789b1b4239e5f/experiments/2b361086488e4955aa2ef6fc221b8945/output/log\n",
      "ClearML Task создана. ID: 2b361086488e4955aa2ef6fc221b8945\n",
      "Исходный файл данных: transaction.csv\n",
      "Размер файла: 82.91 MB\n",
      "MD5 хэш: 5e35dca941603ad2709b18c7373889a8\n",
      "\n",
      "Логирование исходных данных...\n",
      "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n",
      "✓ Оригинальный файл прикреплен как артефакт 'raw_source_data'\n",
      "✓ Исходные данные успешно залогированы в ClearML\n",
      "  Хэш файла: 5e35dca941603ad2709b18c7373889a8\n",
      "  Размер: 82.91 MB\n",
      "Начинаем агрегацию данных клиентов\n",
      "Агрегация завершена. Размер данных: (1003083, 6)\n",
      "Строим профиль клиента на дату: 2019-09-01\n",
      "Профиль построен. Клиентов: 39906\n",
      "Разметка событий завершена. Клиентов с событиями: 8821\n",
      "Создаем обучающую выборку\n",
      "Размер выборки: (39906, 12)\n",
      "Распределение классов: {False: 32375, True: 7531}\n",
      "Тренировочная выборка: (31924, 10)\n",
      "Тестовая выборка: (7982, 10)\n",
      "Начинаем обучение модели RandomForest\n",
      "\n",
      "==================================================\n",
      "МЕТРИКИ КАЧЕСТВА МОДЕЛИ:\n",
      "==================================================\n",
      "Accuracy: 0.7716\n",
      "Precision: 0.4306\n",
      "Recall: 0.6527\n",
      "F1-Score: 0.5189\n",
      "ROC-AUC: 0.8073\n",
      "==================================================\n",
      "Модель сохранена как: random_forest_model.pkl\n",
      "Модель прикреплена как артефакт к задаче ClearML\n",
      "\n",
      "Топ-10 важнейших признаков:\n",
      "               feature  importance\n",
      "0              Recency    0.334116\n",
      "9        share_3_month    0.172827\n",
      "1            Frequency    0.119573\n",
      "5   total_unique_items    0.067973\n",
      "3       total_quantity    0.064807\n",
      "2             Monetary    0.064006\n",
      "8    amount_last_visit    0.056574\n",
      "4            avg_check    0.052341\n",
      "6  avg_items_per_visit    0.043755\n",
      "7       weekend_visits    0.024028\n",
      "Confusion Matrix сохранен как: confusion_matrix.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maris\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\clearml\\utilities\\plotlympl\\renderer.py:209: UserWarning:\n",
      "\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance сохранен как: feature_importance.png\n",
      "ROC Curve сохранена как: roc_curve.png\n",
      "\n",
      "==================================================\n",
      "ЭКСПЕРИМЕНТ УСПЕШНО ЗАВЕРШЕН\n",
      "ID задачи ClearML: 2b361086488e4955aa2ef6fc221b8945\n",
      "Ссылка на эксперимент: https://app.clear.ml/projects/37329c57f7424fb081d789b1b4239e5f/experiments/2b361086488e4955aa2ef6fc221b8945/output/log\n",
      "==================================================\n",
      "\n",
      "ФИНАЛЬНЫЙ ОТЧЕТ:\n",
      "Лучшая метрика (ROC-AUC): 0.8073\n",
      "Accuracy: 0.7716\n",
      "Precision: 0.4306\n",
      "Recall: 0.6527\n",
      "F1-Score: 0.5189\n",
      "\n",
      "Модель сохранена как: random_forest_model.pkl\n",
      "Артефакты сохранены в текущей директории\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cannot write mode RGBA as JPEG\n",
      "cannot write mode RGBA as JPEG\n",
      "cannot write mode RGBA as JPEG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n",
      "ClearML Monitor: Reporting detected, reverting back to iteration based reporting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None)': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None)': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=235, connect=240, read=235, redirect=240, status=240)) after connection broken by 'ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None)': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=234, connect=240, read=234, redirect=240, status=240)) after connection broken by 'ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None)': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=233, connect=240, read=233, redirect=240, status=240)) after connection broken by 'ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None)': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=300.0)\")': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None)': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=232, connect=240, read=232, redirect=240, status=240)) after connection broken by 'ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None)': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.get_all\n",
      "Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.get_all\n",
      "Retrying (Retry(total=231, connect=240, read=231, redirect=240, status=240)) after connection broken by 'ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None)': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.get_all\n",
      "Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.get_all\n",
      "Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.get_all\n",
      "Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=230, connect=240, read=230, redirect=240, status=240)) after connection broken by 'ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None)': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.get_all\n",
      "Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.get_all\n",
      "Retrying (Retry(total=235, connect=240, read=235, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.get_all\n",
      "Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.get_all\n",
      "Retrying (Retry(total=235, connect=240, read=235, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.get_all\n",
      "Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.get_all\n",
      "Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.get_all\n",
      "Retrying (Retry(total=229, connect=240, read=229, redirect=240, status=240)) after connection broken by 'ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None)': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.get_all\n",
      "Retrying (Retry(total=235, connect=240, read=235, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.get_all\n",
      "Retrying (Retry(total=234, connect=240, read=234, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=228, connect=240, read=228, redirect=240, status=240)) after connection broken by 'ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None)': /v2.23/events.add_batch\n",
      "Retrying (Retry(total=235, connect=240, read=235, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.ping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Задача ClearML закрыта успешно\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying (Retry(total=234, connect=240, read=234, redirect=240, status=240)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.clear.ml', port=443): Read timed out. (read timeout=10.0)\")': /v2.23/tasks.ping\n"
     ]
    }
   ],
   "source": [
    "# Добавьте в начало файла установку ClearML и импорт\n",
    "# В Jupyter Notebook сначала выполните:\n",
    "# !pip install clearml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import pickle\n",
    "import joblib\n",
    "# Добавьте этот импорт\n",
    "from PIL import Image\n",
    "\n",
    "# ========== CLEARML ИНИЦИАЛИЗАЦИЯ ==========\n",
    "from clearml import Task\n",
    "\n",
    "# Создаем задачу ClearML\n",
    "task = Task.init(\n",
    "    project_name=\"Customer_Purchase_Prediction\",\n",
    "    task_name=\"RandomForest_Prediction_Experiment\",\n",
    "    task_type=Task.TaskTypes.training,\n",
    "    reuse_last_task_id=False\n",
    ")\n",
    "\n",
    "# Логируем информацию о задаче\n",
    "print(\"ClearML Task создана. ID:\", task.id)\n",
    "\n",
    "# ========== ЛОГИРОВАНИЕ ИСХОДНЫХ ДАННЫХ КАК АРТЕФАКТ ==========\n",
    "import os\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "# Путь к исходному файлу данных\n",
    "source_file_path = 'C:/Users/maris/OneDrive/Рабочий стол/transaction.csv'\n",
    "\n",
    "# Создаем копию исходного файла с метаданными\n",
    "def prepare_data_artifact(file_path):\n",
    "    \"\"\"\n",
    "    Подготавливает и логирует исходные данные как артефакт ClearML\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Проверяем существование файла\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Внимание: Файл {file_path} не найден!\")\n",
    "            return None\n",
    "        \n",
    "        # Читаем файл для получения информации о нем\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        file_name = os.path.basename(file_path)\n",
    "        file_extension = os.path.splitext(file_path)[1]\n",
    "        \n",
    "        # Вычисляем хэш файла для проверки целостности\n",
    "        def calculate_file_hash(filepath):\n",
    "            hash_md5 = hashlib.md5()\n",
    "            with open(filepath, \"rb\") as f:\n",
    "                for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                    hash_md5.update(chunk)\n",
    "            return hash_md5.hexdigest()\n",
    "        \n",
    "        file_hash = calculate_file_hash(file_path)\n",
    "        \n",
    "        # Создаем метаданные файла\n",
    "        metadata = {\n",
    "            'filename': file_name,\n",
    "            'file_path': file_path,\n",
    "            'file_size_bytes': file_size,\n",
    "            'file_size_mb': round(file_size / (1024 * 1024), 2),\n",
    "            'file_extension': file_extension,\n",
    "            'file_hash_md5': file_hash,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'rows_count': None,\n",
    "            'columns_count': None,\n",
    "            'columns': None\n",
    "        }\n",
    "        \n",
    "        # Читаем первые несколько строк для предварительного анализа\n",
    "        try:\n",
    "            df_preview = pd.read_csv(file_path, nrows=5)\n",
    "            metadata['rows_count_preview'] = len(df_preview)\n",
    "            metadata['columns_count'] = len(df_preview.columns)\n",
    "            metadata['columns'] = list(df_preview.columns)\n",
    "            \n",
    "            # Сохраняем предпросмотр в отдельный файл\n",
    "            preview_file = 'data_preview.csv'\n",
    "            df_preview.to_csv(preview_file, index=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Предварительный анализ файла не удался: {e}\")\n",
    "        \n",
    "        # Логируем метаданные\n",
    "        task.get_logger().report_text(f\"Исходный файл данных: {file_name}\")\n",
    "        task.get_logger().report_text(f\"Размер файла: {metadata['file_size_mb']} MB\")\n",
    "        task.get_logger().report_text(f\"MD5 хэш: {file_hash}\")\n",
    "        \n",
    "        # Сохраняем метаданные в файл\n",
    "        import json\n",
    "        metadata_file = 'data_metadata.json'\n",
    "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Логируем файл как артефакт\n",
    "        print(f\"\\nЛогирование исходных данных...\")\n",
    "        \n",
    "        # Способ 1: Прикрепляем оригинальный файл (если он не слишком большой)\n",
    "        if file_size < 100 * 1024 * 1024:  # Если файл меньше 100MB\n",
    "            task.upload_artifact(\n",
    "                name='raw_source_data',\n",
    "                artifact_object=file_path,\n",
    "                metadata=metadata\n",
    "            )\n",
    "            print(f\"✓ Оригинальный файл прикреплен как артефакт 'raw_source_data'\")\n",
    "        else:\n",
    "            print(f\"⚠ Файл слишком большой ({metadata['file_size_mb']} MB), создаем сэмпл\")\n",
    "            # Создаем сэмпл данных (первые 10,000 строк)\n",
    "            df_sample = pd.read_csv(file_path, nrows=10000)\n",
    "            sample_file = 'data_sample_10000_rows.csv'\n",
    "            df_sample.to_csv(sample_file, index=False)\n",
    "            \n",
    "            task.upload_artifact(\n",
    "                name='data_sample',\n",
    "                artifact_object=sample_file,\n",
    "                metadata={**metadata, 'sample_rows': 10000}\n",
    "            )\n",
    "            print(f\"✓ Сэмпл данных (10,000 строк) прикреплен как артефакт 'data_sample'\")\n",
    "        \n",
    "        # Способ 2: Прикрепляем метаданные и предпросмотр\n",
    "        task.upload_artifact(\n",
    "            name='data_metadata',\n",
    "            artifact_object=metadata_file,\n",
    "            metadata={'description': 'Метаданные исходного файла данных'}\n",
    "        )\n",
    "        \n",
    "        if 'preview_file' in locals():\n",
    "            task.upload_artifact(\n",
    "                name='data_preview',\n",
    "                artifact_object=preview_file,\n",
    "                metadata={'description': 'Предпросмотр первых 5 строк данных'}\n",
    "            )\n",
    "        \n",
    "        # Сохраняем путь к файлу в конфигурации задачи\n",
    "        task.connect_configuration(\n",
    "            name='data_source',\n",
    "            configuration={\n",
    "                'source_file': file_path,\n",
    "                'file_hash': file_hash,\n",
    "                'upload_timestamp': metadata['timestamp']\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Исходные данные успешно залогированы в ClearML\")\n",
    "        print(f\"  Хэш файла: {file_hash}\")\n",
    "        print(f\"  Размер: {metadata['file_size_mb']} MB\")\n",
    "        \n",
    "        return metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при логировании исходных данных: {e}\")\n",
    "        return None\n",
    "\n",
    "# Вызываем функцию логирования данных\n",
    "data_metadata = prepare_data_artifact(source_file_path)\n",
    "\n",
    "# ========== ПРОДОЛЖАЕМ С ЗАГРУЗКОЙ ДАННЫХ ==========\n",
    "\n",
    "# ========== ВАШ СУЩЕСТВУЮЩИЙ КОД ==========\n",
    "# Загружаем таблицу из файла в переменную df (data frame) и переименовываем колонки\n",
    "df = pd.read_csv('C:/Users/maris/OneDrive/Рабочий стол/transaction.csv')\n",
    "\n",
    "rename_dict = {\n",
    "    'clientID': 'client',\n",
    "    'trDte': 'visit_date', \n",
    "    'itemGroup': 'item_group'\n",
    "}\n",
    "\n",
    "df = df.rename(columns=rename_dict)\n",
    "# Преобразование даты (уже с новым именем)\n",
    "df['visit_date'] = pd.to_datetime(df['visit_date'], format='%d.%m.%Y')\n",
    "\n",
    "# ========== ОБНОВЛЕННАЯ ФУНКЦИЯ С ЛОГИРОВАНИЕМ ==========\n",
    "def aggregate_client_daily_items(df):\n",
    "    \"\"\"\n",
    "    Функция группирует покупки по клиенту, дате, товару и группе.\n",
    "    Суммирует количество и сумму. Возвращает отсортированную таблицу.\n",
    "    \"\"\"\n",
    "    # Логируем начало обработки данных\n",
    "    task.get_logger().report_text(\"Начинаем агрегацию данных клиентов\")\n",
    "    \n",
    "    # Группируем строки по: клиенту, дате, товару, группе товара\n",
    "    grouped = df.groupby(\n",
    "        ['client', 'visit_date', 'item', 'item_group'],  # по этим колонкам объединяем\n",
    "        as_index=False  # чтобы client и другие не стали индексом\n",
    "    ).agg({\n",
    "        'quantity': 'sum',  # складываем количество\n",
    "        'amount': 'sum'     # складываем сумму\n",
    "    })\n",
    "    \n",
    "    # Переименовываем колонку visit_date в visit_date, как требуется\n",
    "    grouped = grouped.rename(columns={'visit_date': 'visit_date'})\n",
    "    \n",
    "    # Сортируем результат по client, потом по дате, потом по товару\n",
    "    result = grouped.sort_values(by=['client', 'visit_date', 'item'])\n",
    "    \n",
    "    # Сбрасываем номера строк, чтобы они шли по порядку (0, 1, 2...)\n",
    "    result = result.reset_index(drop=True)\n",
    "    \n",
    "    # Логируем размер результата\n",
    "    task.get_logger().report_text(f\"Агрегация завершена. Размер данных: {result.shape}\")\n",
    "    \n",
    "    # Возвращаем итоговую таблицу\n",
    "    return result\n",
    "\n",
    "result_df = aggregate_client_daily_items(df)\n",
    "\n",
    "# ========== ДОПОЛНИТЕЛЬНЫЕ ФУНКЦИИ ==========\n",
    "def calculate_client_profile_at_date(visits_df, observation_end_date):\n",
    "    \"\"\"\n",
    "    Строит профиль клиента на дату observation_end_date (исключая её).\n",
    "    Нет утечки будущего — только данные СТРОГО ДО этой даты.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Логируем параметры функции\n",
    "    task.get_logger().report_text(f\"Строим профиль клиента на дату: {observation_end_date}\")\n",
    "    \n",
    "    # ... (ваш существующий код функции calculate_client_profile_at_date)\n",
    "    obs_date = pd.to_datetime(observation_end_date)\n",
    "    \n",
    "    df = visits_df.copy()\n",
    "    df['visit_date'] = pd.to_datetime(df['visit_date'])\n",
    "    \n",
    "    # Оставляем только визиты СТРОГО ДО observation_end_date\n",
    "    filtered_df = df[df['visit_date'] < obs_date]\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        return pd.DataFrame(columns=[\n",
    "            'client', 'Recency', 'Frequency', 'Monetary', 'last_visit_date',\n",
    "            'total_quantity', 'avg_check', 'total_unique_items',\n",
    "            'avg_items_per_visit', 'weekend_visits', 'amount_last_visit',\n",
    "            'share_3_month'\n",
    "        ])\n",
    "    \n",
    "    profiles = []\n",
    "    \n",
    "    for client_id, group in filtered_df.groupby('client'):\n",
    "        # ... (ваш существующий код обработки клиентов)\n",
    "        group = group.sort_values(by='visit_date')\n",
    "        visit_dates = group['visit_date'].drop_duplicates().sort_values()\n",
    "        n_visits = len(visit_dates)\n",
    "        total_monetary = group['amount'].sum()\n",
    "        total_quantity = group['quantity'].sum()\n",
    "        last_visit = visit_dates.max()\n",
    "        recency_days = (obs_date - last_visit).days\n",
    "        frequency = n_visits\n",
    "        monetary = total_monetary\n",
    "        last_visit_date = last_visit.strftime('%Y-%m-%d')\n",
    "        avg_check = monetary / frequency if frequency > 0 else 0\n",
    "        total_unique_items = group['item'].nunique()\n",
    "        avg_items_per_visit = total_quantity / frequency if frequency > 0 else 0\n",
    "        \n",
    "        visit_days = visit_dates.dt.weekday\n",
    "        weekend_count = visit_days.isin([5, 6]).sum()\n",
    "        weekend_visits = weekend_count / frequency if frequency > 0 else 0\n",
    "        \n",
    "        last_visit_date_val = visit_dates.max()\n",
    "        last_visit_rows = group[group['visit_date'] == last_visit_date_val]\n",
    "        amount_last_visit_value = last_visit_rows['amount'].sum()\n",
    "        amount_last_visit = (\n",
    "            amount_last_visit_value / total_monetary\n",
    "            if total_monetary > 0 else 0\n",
    "        )\n",
    "        \n",
    "        three_months_ago = obs_date - pd.Timedelta(days=90)\n",
    "        recent_visits = group[group['visit_date'] >= three_months_ago]\n",
    "        amount_last_3m = recent_visits['amount'].sum()\n",
    "        share_3_month = amount_last_3m / total_monetary if total_monetary > 0 else 0\n",
    "        \n",
    "        profile = {\n",
    "            'client': client_id,\n",
    "            'Recency': recency_days,\n",
    "            'Frequency': frequency,\n",
    "            'Monetary': monetary,\n",
    "            'last_visit_date': last_visit_date,\n",
    "            'total_quantity': total_quantity,\n",
    "            'avg_check': round(avg_check, 2),\n",
    "            'total_unique_items': total_unique_items,\n",
    "            'avg_items_per_visit': round(avg_items_per_visit, 2),\n",
    "            'weekend_visits': round(weekend_visits, 3),\n",
    "            'amount_last_visit': round(amount_last_visit, 3),\n",
    "            'share_3_month': round(share_3_month, 3)\n",
    "        }\n",
    "        \n",
    "        profiles.append(profile)\n",
    "    \n",
    "    result_df = pd.DataFrame(profiles)\n",
    "    result_df = result_df.sort_values(by='client').reset_index(drop=True)\n",
    "    \n",
    "    task.get_logger().report_text(f\"Профиль построен. Клиентов: {len(result_df)}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "result_profile = calculate_client_profile_at_date(\n",
    "    visits_df=result_df, \n",
    "    observation_end_date='2019-09-01'\n",
    ")\n",
    "\n",
    "# ========== ФУНКЦИЯ ДЛЯ СОЗДАНИЯ ТРЕНИРОВОЧНОЙ ВЫБОРКИ ==========\n",
    "def mark_events(visits_df, result_start_date, result_end_date):\n",
    "    \"\"\"\n",
    "    Размечает, какие клиенты посетили магазин в указанном периоде.\n",
    "    \"\"\"\n",
    "    # ... (ваш существующий код функции mark_events)\n",
    "    start_date = pd.to_datetime(result_start_date)\n",
    "    end_date = pd.to_datetime(result_end_date)\n",
    "    \n",
    "    df = visits_df.copy()\n",
    "    df['visit_date'] = pd.to_datetime(df['visit_date'])\n",
    "    \n",
    "    all_clients = df['client'].unique()\n",
    "    result = pd.DataFrame({'client': all_clients})\n",
    "    result['event'] = False\n",
    "    \n",
    "    mask = (df['visit_date'] >= start_date) & (df['visit_date'] < end_date)\n",
    "    filtered_visits = df[mask]\n",
    "    clients_with_visits = filtered_visits['client'].unique()\n",
    "    \n",
    "    result.loc[result['client'].isin(clients_with_visits), 'event'] = True\n",
    "    result = result.sort_values(by='client').reset_index(drop=True)\n",
    "    \n",
    "    task.get_logger().report_text(f\"Разметка событий завершена. Клиентов с событиями: {len(clients_with_visits)}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "events = mark_events(\n",
    "    visits_df=result_df,\n",
    "    result_start_date='2019-09-01',\n",
    "    result_end_date='2019-10-01'\n",
    ")\n",
    "\n",
    "# ========== ФУНКЦИЯ СОЗДАНИЯ ТРЕНИРОВОЧНОЙ ВЫБОРКИ ==========\n",
    "def create_training_sample(profile_df, events_df):\n",
    "    \"\"\"\n",
    "    Создаёт обучающую выборку, соединяя профили клиентов и разметку события.\n",
    "    \"\"\"\n",
    "    # Логируем создание выборки\n",
    "    task.get_logger().report_text(\"Создаем обучающую выборку\")\n",
    "    \n",
    "    merged = pd.merge(profile_df, events_df, on='client', how='inner')\n",
    "    \n",
    "    if 'event' not in merged.columns:\n",
    "        raise ValueError(\"После объединения нет колонки 'event'\")\n",
    "    \n",
    "    if merged['event'].isnull().any():\n",
    "        raise ValueError(\"В колонке 'event' есть пропущенные значения\")\n",
    "    \n",
    "    columns_to_drop = ['last_visit_date']\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in merged.columns]\n",
    "    merged = merged.drop(columns=columns_to_drop)\n",
    "    \n",
    "    event_values = merged['event'].unique()\n",
    "    if not (True in event_values and False in event_values):\n",
    "        raise ValueError(\"В выборке нет обоих классов (True и False). Модель не сможет обучиться.\")\n",
    "    \n",
    "    merged = merged.sort_values(by='client').reset_index(drop=True)\n",
    "    \n",
    "    # Логируем информацию о выборке\n",
    "    task.get_logger().report_text(f\"Размер выборки: {merged.shape}\")\n",
    "    task.get_logger().report_text(f\"Распределение классов: {merged['event'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return merged\n",
    "\n",
    "sample = create_training_sample(result_profile, events)\n",
    "\n",
    "# ========== ПОДГОТОВКА ДАННЫХ ДЛЯ МОДЕЛИ ==========\n",
    "# Разделяем данные на признаки и целевую переменную\n",
    "X = sample.drop(['client', 'event'], axis=1)\n",
    "y = sample['event']\n",
    "\n",
    "# Разделяем на тренировочную и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Логируем размеры выборок\n",
    "task.get_logger().report_text(f\"Тренировочная выборка: {X_train.shape}\")\n",
    "task.get_logger().report_text(f\"Тестовая выборка: {X_test.shape}\")\n",
    "\n",
    "# ========== ОБУЧЕНИЕ МОДЕЛИ ==========\n",
    "task.get_logger().report_text(\"Начинаем обучение модели RandomForest\")\n",
    "\n",
    "# Создаем и обучаем модель\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ========== ПРЕДСКАЗАНИЯ И МЕТРИКИ ==========\n",
    "# Делаем предсказания\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Вычисляем метрики\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# ========== ЛОГИРОВАНИЕ МЕТРИК В CLEARML ==========\n",
    "task.get_logger().report_scalar(\"Accuracy\", \"Test\", accuracy, iteration=0)\n",
    "task.get_logger().report_scalar(\"Precision\", \"Test\", precision, iteration=0)\n",
    "task.get_logger().report_scalar(\"Recall\", \"Test\", recall, iteration=0)\n",
    "task.get_logger().report_scalar(\"F1-Score\", \"Test\", f1, iteration=0)\n",
    "task.get_logger().report_scalar(\"ROC-AUC\", \"Test\", roc_auc, iteration=0)\n",
    "\n",
    "# Выводим метрики в консоль\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"МЕТРИКИ КАЧЕСТВА МОДЕЛИ:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ========== СОХРАНЕНИЕ МОДЕЛИ ==========\n",
    "# Сохраняем модель в формате pickle\n",
    "model_filename = 'random_forest_model.pkl'\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Прикрепляем модель как артефакт к задаче ClearML\n",
    "task.upload_artifact(name='trained_model', artifact_object=model_filename)\n",
    "\n",
    "# Также сохраняем с помощью joblib (альтернатива)\n",
    "joblib.dump(model, 'random_forest_model.joblib')\n",
    "task.upload_artifact(name='trained_model_joblib', artifact_object='random_forest_model.joblib')\n",
    "\n",
    "print(f\"Модель сохранена как: {model_filename}\")\n",
    "print(f\"Модель прикреплена как артефакт к задаче ClearML\")\n",
    "\n",
    "# ========== ДОПОЛНИТЕЛЬНАЯ ИНФОРМАЦИЯ ==========\n",
    "# Логируем важность признаков\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nТоп-10 важнейших признаков:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Сохраняем важность признаков как артефакт\n",
    "feature_importance.to_csv('feature_importance.csv', index=False)\n",
    "task.upload_artifact(name='feature_importance', artifact_object='feature_importance.csv')\n",
    "\n",
    "# ========== СОХРАНЕНИЕ РЕЗУЛЬТАТОВ ==========\n",
    "# Сохраняем результаты предсказаний\n",
    "results_df = X_test.copy()\n",
    "results_df['actual'] = y_test.values\n",
    "results_df['predicted'] = y_pred\n",
    "results_df['predicted_proba'] = y_pred_proba\n",
    "\n",
    "results_df.to_csv('prediction_results.csv', index=False)\n",
    "task.upload_artifact(name='prediction_results', artifact_object='prediction_results.csv')\n",
    "\n",
    "# ========== ВИЗУАЛИЗАЦИЯ И ЛОГИРОВАНИЕ ==========\n",
    "# Создаем confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Сохраняем график\n",
    "plt.tight_layout()\n",
    "confusion_matrix_path = 'confusion_matrix.png'\n",
    "plt.savefig(confusion_matrix_path, dpi=100)\n",
    "plt.close()\n",
    "\n",
    "# Загружаем изображение и логируем его\n",
    "try:\n",
    "    from PIL import Image\n",
    "    img = Image.open(confusion_matrix_path)\n",
    "    \n",
    "    # Логируем изображение в ClearML\n",
    "    if task:\n",
    "        task.get_logger().report_image(\n",
    "            title=\"Confusion Matrix\",\n",
    "            series=\"Test\",\n",
    "            image=img\n",
    "        )\n",
    "    print(f\"Confusion Matrix сохранен как: {confusion_matrix_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при сохранении изображения: {e}\")\n",
    "\n",
    "# Также создаем график важности признаков\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Важность признака')\n",
    "plt.title('Топ-15 важнейших признаков')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "\n",
    "feature_importance_path = 'feature_importance.png'\n",
    "plt.savefig(feature_importance_path, dpi=100)\n",
    "plt.close()\n",
    "\n",
    "# Логируем график важности признаков\n",
    "try:\n",
    "    img2 = Image.open(feature_importance_path)\n",
    "    if task:\n",
    "        task.get_logger().report_image(\n",
    "            title=\"Feature Importance\",\n",
    "            series=\"Analysis\",\n",
    "            image=img2\n",
    "        )\n",
    "    print(f\"Feature Importance сохранен как: {feature_importance_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при сохранении графика важности признаков: {e}\")\n",
    "\n",
    "# Создаем ROC-AUC кривую (опционально)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC кривая (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "roc_curve_path = 'roc_curve.png'\n",
    "plt.savefig(roc_curve_path, dpi=100)\n",
    "plt.close()\n",
    "\n",
    "# Логируем ROC кривую\n",
    "try:\n",
    "    img3 = Image.open(roc_curve_path)\n",
    "    if task:\n",
    "        task.get_logger().report_image(\n",
    "            title=\"ROC Curve\",\n",
    "            series=\"Test\",\n",
    "            image=img3\n",
    "        )\n",
    "    print(f\"ROC Curve сохранена как: {roc_curve_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при сохранении ROC кривой: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ЭКСПЕРИМЕНТ УСПЕШНО ЗАВЕРШЕН\")\n",
    "if task:\n",
    "    print(f\"ID задачи ClearML: {task.id}\")\n",
    "    print(f\"Ссылка на эксперимент: {task.get_output_log_web_page()}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Создаем финальный отчет\n",
    "print(\"\\nФИНАЛЬНЫЙ ОТЧЕТ:\")\n",
    "print(f\"Лучшая метрика (ROC-AUC): {roc_auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nМодель сохранена как: {model_filename}\")\n",
    "print(f\"Артефакты сохранены в текущей директории\")\n",
    "\n",
    "# Завершаем задачу\n",
    "if task:\n",
    "    task.close()\n",
    "    print(\"\\nЗадача ClearML закрыта успешно\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
